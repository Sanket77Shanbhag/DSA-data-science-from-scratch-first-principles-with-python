{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cc86c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c22faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import math\n",
    "\n",
    "def entropy(class_probabilities: List[float]) -> float:\n",
    "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2)\n",
    "               for p in class_probabilities\n",
    "               if p > 0)                     # ignore zero probabilities\n",
    "\n",
    "assert entropy([1.0]) == 0\n",
    "assert entropy([0.5, 0.5]) == 1\n",
    "assert 0.81 < entropy([0.25, 0.75]) < 0.82\n",
    "\n",
    "from typing import Any\n",
    "from collections import Counter\n",
    "\n",
    "def class_probabilities(labels: List[Any]) -> List[float]:\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count\n",
    "            for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labels: List[Any]) -> float:\n",
    "    return entropy(class_probabilities(labels))\n",
    "\n",
    "assert data_entropy(['a']) == 0\n",
    "assert data_entropy([True, False]) == 1\n",
    "assert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])\n",
    "\n",
    "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
    "    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "\n",
    "    return sum(data_entropy(subset) * len(subset) / total_count\n",
    "               for subset in subsets)\n",
    "\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "class Candidate(NamedTuple):\n",
    "    level: str\n",
    "    lang: str\n",
    "    tweets: bool\n",
    "    phd: bool\n",
    "    did_well: Optional[bool] = None  # allow unlabeled data\n",
    "\n",
    "                  #  level     lang     tweets  phd  did_well\n",
    "inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
    "          Candidate('Senior', 'Java',   False, True,  False),\n",
    "          Candidate('Mid',    'Python', False, False, True),\n",
    "          Candidate('Junior', 'Python', False, False, True),\n",
    "          Candidate('Junior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'R',      True,  True,  False),\n",
    "          Candidate('Mid',    'R',      True,  True,  True),\n",
    "          Candidate('Senior', 'Python', False, False, False),\n",
    "          Candidate('Senior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'Python', True,  False, True),\n",
    "          Candidate('Senior', 'Python', True,  True,  True),\n",
    "          Candidate('Mid',    'Python', False, True,  True),\n",
    "          Candidate('Mid',    'Java',   True,  False, True),\n",
    "          Candidate('Junior', 'Python', False, True,  False)\n",
    "         ]\n",
    "\n",
    "from typing import Dict, TypeVar\n",
    "from collections import defaultdict\n",
    "\n",
    "T = TypeVar('T')  # generic type for inputs\n",
    "\n",
    "def partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]:\n",
    "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
    "    partitions: Dict[Any, List[T]] = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = getattr(input, attribute)  # value of the specified attribute\n",
    "        partitions[key].append(input)    # add input to the correct partition\n",
    "    return partitions\n",
    "\n",
    "def partition_entropy_by(inputs: List[Any],\n",
    "                         attribute: str,\n",
    "                         label_attribute: str) -> float:\n",
    "    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n",
    "    # partitions consist of our inputs\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "\n",
    "    # but partition_entropy needs just the class labels\n",
    "    labels = [[getattr(input, label_attribute) for input in partition]\n",
    "              for partition in partitions.values()]\n",
    "\n",
    "    return partition_entropy(labels)\n",
    "\n",
    "for key in ['level','lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by(inputs, key, 'did_well'))\n",
    "\n",
    "assert 0.69 < partition_entropy_by(inputs, 'level', 'did_well')  < 0.70\n",
    "assert 0.86 < partition_entropy_by(inputs, 'lang', 'did_well')   < 0.87\n",
    "assert 0.78 < partition_entropy_by(inputs, 'tweets', 'did_well') < 0.79\n",
    "assert 0.89 < partition_entropy_by(inputs, 'phd', 'did_well')    < 0.90\n",
    "\n",
    "senior_inputs = [input for input in inputs if input.level == 'Senior']\n",
    "\n",
    "assert 0.4 == partition_entropy_by(senior_inputs, 'lang', 'did_well')\n",
    "assert 0.0 == partition_entropy_by(senior_inputs, 'tweets', 'did_well')\n",
    "assert 0.95 < partition_entropy_by(senior_inputs, 'phd', 'did_well') < 0.96\n",
    "\n",
    "from typing import NamedTuple, Union, Any\n",
    "\n",
    "class Leaf(NamedTuple):\n",
    "    value: Any\n",
    "\n",
    "class Split(NamedTuple):\n",
    "    attribute: str\n",
    "    subtrees: dict\n",
    "    default_value: Any = None\n",
    "\n",
    "DecisionTree = Union[Leaf, Split]\n",
    "\n",
    "hiring_tree = Split('level', {   # First, consider \"level\".\n",
    "    'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\"\n",
    "        False: Leaf(True),       #   if \"phd\" is False, predict True\n",
    "        True: Leaf(False)        #   if \"phd\" is True, predict False\n",
    "    }),\n",
    "    'Mid': Leaf(True),           # if level is \"Mid\", just predict True\n",
    "    'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\"\n",
    "        False: Leaf(False),      #   if \"tweets\" is False, predict False\n",
    "        True: Leaf(True)         #   if \"tweets\" is True, predict True\n",
    "    })\n",
    "})\n",
    "\n",
    "def classify(tree: DecisionTree, input: Any) -> Any:\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "\n",
    "    # If this is a leaf node, return its value\n",
    "    if isinstance(tree, Leaf):\n",
    "        return tree.value\n",
    "\n",
    "    # Otherwise this tree consists of an attribute to split on\n",
    "    # and a dictionary whose keys are values of that attribute\n",
    "    # and whose values of are subtrees to consider next\n",
    "    subtree_key = getattr(input, tree.attribute)\n",
    "\n",
    "    if subtree_key not in tree.subtrees:   # If no subtree for key,\n",
    "        return tree.default_value          # return the default value.\n",
    "\n",
    "    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree\n",
    "    return classify(subtree, input)        # and use it to classify the input.\n",
    "\n",
    "def build_tree_id3(inputs: List[Any],\n",
    "                   split_attributes: List[str],\n",
    "                   target_attribute: str) -> DecisionTree:\n",
    "    # Count target labels\n",
    "    label_counts = Counter(getattr(input, target_attribute)\n",
    "                           for input in inputs)\n",
    "    most_common_label = label_counts.most_common(1)[0][0]\n",
    "\n",
    "    # If there's a unique label, predict it\n",
    "    if len(label_counts) == 1:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # If no split attributes left, return the majority label\n",
    "    if not split_attributes:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # Otherwise split by the best attribute\n",
    "\n",
    "    def split_entropy(attribute: str) -> float:\n",
    "        \"\"\"Helper function for finding the best attribute\"\"\"\n",
    "        return partition_entropy_by(inputs, attribute, target_attribute)\n",
    "\n",
    "    best_attribute = min(split_attributes, key=split_entropy)\n",
    "\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_attributes = [a for a in split_attributes if a != best_attribute]\n",
    "\n",
    "    # recursively build the subtrees\n",
    "    subtrees = {attribute_value : build_tree_id3(subset,\n",
    "                                                 new_attributes,\n",
    "                                                 target_attribute)\n",
    "                for attribute_value, subset in partitions.items()}\n",
    "\n",
    "    return Split(best_attribute, subtrees, default_value=most_common_label)\n",
    "\n",
    "tree = build_tree_id3(inputs,\n",
    "                      ['level', 'lang', 'tweets', 'phd'],\n",
    "                      'did_well')\n",
    "\n",
    "# Should predict True\n",
    "assert classify(tree, Candidate(\"Junior\", \"Java\", True, False))\n",
    "\n",
    "# Should predict False\n",
    "assert not classify(tree, Candidate(\"Junior\", \"Java\", True, True))\n",
    "\n",
    "# Should predict True\n",
    "assert classify(tree, Candidate(\"Intern\", \"Java\", True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9332e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(classify(tree, Candidate(\"Junior\", \"Java\", True, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6865686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(classify(tree, Candidate(\"Junior\", \"Java\", True, True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5860e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(classify(tree, Candidate(\"Intern\", \"Java\", True, True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f986c14-ceb7-4e14-bd9d-707741fdd6fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c937f2a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 236\u001b[0m\n\u001b[0;32m    232\u001b[0m             num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28mprint\u001b[39m(num_correct, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: main()\n",
      "Cell \u001b[1;32mIn[5], line 170\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m network \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;66;03m# hidden layer: 2 inputs -> 2 outputs\u001b[39;00m\n\u001b[0;32m    163\u001b[0m             [[random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)],   \u001b[38;5;66;03m# 1st hidden neuron\u001b[39;00m\n\u001b[0;32m    164\u001b[0m              [random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]],  \u001b[38;5;66;03m# 2nd hidden neuron\u001b[39;00m\n\u001b[0;32m    165\u001b[0m             \u001b[38;5;66;03m# output layer: 2 inputs -> 1 output\u001b[39;00m\n\u001b[0;32m    166\u001b[0m             [[random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]]   \u001b[38;5;66;03m# 1st output neuron\u001b[39;00m\n\u001b[0;32m    167\u001b[0m           ]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscratch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgradient_descent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gradient_step\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[0;32m    172\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtrange(\u001b[38;5;241m20000\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneural net for xor\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from scratch.linear_algebra import Vector, dot\n",
    "\n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "    \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)\n",
    "\n",
    "and_weights = [2., 2]\n",
    "and_bias = -3.\n",
    "\n",
    "assert perceptron_output(and_weights, and_bias, [1, 1]) == 1\n",
    "assert perceptron_output(and_weights, and_bias, [0, 1]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [1, 0]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [0, 0]) == 0\n",
    "\n",
    "or_weights = [2., 2]\n",
    "or_bias = -1.\n",
    "\n",
    "assert perceptron_output(or_weights, or_bias, [1, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [1, 0]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 0]) == 0\n",
    "\n",
    "not_weights = [-2.]\n",
    "not_bias = 1.\n",
    "\n",
    "assert perceptron_output(not_weights, not_bias, [0]) == 1\n",
    "assert perceptron_output(not_weights, not_bias, [1]) == 0\n",
    "\n",
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "    # weights includes the bias term, inputs includes a 1\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                 input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # Add a constant.\n",
    "        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
    "                  for neuron in layer]                    # for each neuron.\n",
    "        outputs.append(output)                            # Add to results.\n",
    "\n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n",
    "\n",
    "xor_network = [# hidden layer\n",
    "               [[20., 20, -30],      # 'and' neuron\n",
    "                [20., 20, -10]],     # 'or'  neuron\n",
    "               # output layer\n",
    "               [[-60., 60, -30]]]    # '2nd input but not 1st input' neuron\n",
    "\n",
    "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
    "# final output, and the [0] gets the value out of the resulting vector\n",
    "assert 0.000 < feed_forward(xor_network, [0, 0])[-1][0] < 0.001\n",
    "assert 0.999 < feed_forward(xor_network, [1, 0])[-1][0] < 1.000\n",
    "assert 0.999 < feed_forward(xor_network, [0, 1])[-1][0] < 1.000\n",
    "assert 0.000 < feed_forward(xor_network, [1, 1])[-1][0] < 0.001\n",
    "\n",
    "def sqerror_gradients(network: List[List[Vector]],\n",
    "                      input_vector: Vector,\n",
    "                      target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                         dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]\n",
    "\n",
    "[   # hidden layer\n",
    "    [[7, 7, -3],     # computes OR\n",
    "     [5, 5, -8]],    # computes AND\n",
    "    # output layer\n",
    "    [[11, -12, -5]]  # computes \"first but not second\"\n",
    "]\n",
    "\n",
    "def fizz_buzz_encode(x: int) -> Vector:\n",
    "    if x % 15 == 0:\n",
    "        return [0, 0, 0, 1]\n",
    "    elif x % 5 == 0:\n",
    "        return [0, 0, 1, 0]\n",
    "    elif x % 3 == 0:\n",
    "        return [0, 1, 0, 0]\n",
    "    else:\n",
    "        return [1, 0, 0, 0]\n",
    "\n",
    "assert fizz_buzz_encode(2) == [1, 0, 0, 0]\n",
    "assert fizz_buzz_encode(6) == [0, 1, 0, 0]\n",
    "assert fizz_buzz_encode(10) == [0, 0, 1, 0]\n",
    "assert fizz_buzz_encode(30) == [0, 0, 0, 1]\n",
    "\n",
    "def binary_encode(x: int) -> Vector:\n",
    "    binary: List[float] = []\n",
    "\n",
    "    for i in range(10):\n",
    "        binary.append(x % 2)\n",
    "        x = x // 2\n",
    "\n",
    "    return binary\n",
    "\n",
    "#                             1  2  4  8 16 32 64 128 256 512\n",
    "assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0]\n",
    "assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]\n",
    "\n",
    "def argmax(xs: list) -> int:\n",
    "    \"\"\"Returns the index of the largest value\"\"\"\n",
    "    return max(range(len(xs)), key=lambda i: xs[i])\n",
    "\n",
    "assert argmax([0, -1]) == 0               # items[0] is largest\n",
    "assert argmax([-1, 0]) == 1               # items[1] is largest\n",
    "assert argmax([-1, 10, 5, 20, -3]) == 3   # items[3] is largest\n",
    "\n",
    "def main():\n",
    "    import random\n",
    "    random.seed(0)\n",
    "   \n",
    "    # training data\n",
    "    xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "    ys = [[0.], [1.], [1.], [0.]]\n",
    "   \n",
    "    # start with random weights\n",
    "    network = [ # hidden layer: 2 inputs -> 2 outputs\n",
    "                [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
    "                 [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
    "                # output layer: 2 inputs -> 1 output\n",
    "                [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
    "              ]\n",
    "   \n",
    "    from scratch.gradient_descent import gradient_step\n",
    "    import tqdm\n",
    "   \n",
    "    learning_rate = 1.0\n",
    "   \n",
    "    for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
    "        for x, y in zip(xs, ys):\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "   \n",
    "            # Take a gradient step for each neuron in each layer\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                        for neuron, grad in zip(layer, layer_grad)]\n",
    "                       for layer, layer_grad in zip(network, gradients)]\n",
    "   \n",
    "    # check that it learned XOR\n",
    "    assert feed_forward(network, [0, 0])[-1][0] < 0.01\n",
    "    assert feed_forward(network, [0, 1])[-1][0] > 0.99\n",
    "    assert feed_forward(network, [1, 0])[-1][0] > 0.99\n",
    "    assert feed_forward(network, [1, 1])[-1][0] < 0.01\n",
    "   \n",
    "    xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "    ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
    "   \n",
    "    NUM_HIDDEN = 25\n",
    "   \n",
    "    network = [\n",
    "        # hidden layer: 10 inputs -> NUM_HIDDEN outputs\n",
    "        [[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)],\n",
    "   \n",
    "        # output_layer: NUM_HIDDEN inputs -> 4 outputs\n",
    "        [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]\n",
    "    ]\n",
    "   \n",
    "    from scratch.linear_algebra import squared_distance\n",
    "   \n",
    "    learning_rate = 1.0\n",
    "   \n",
    "    with tqdm.trange(500) as t:\n",
    "        for epoch in t:\n",
    "            epoch_loss = 0.0\n",
    "   \n",
    "            for x, y in zip(xs, ys):\n",
    "                predicted = feed_forward(network, x)[-1]\n",
    "                epoch_loss += squared_distance(predicted, y)\n",
    "                gradients = sqerror_gradients(network, x, y)\n",
    "   \n",
    "                # Take a gradient step for each neuron in each layer\n",
    "                network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                            for neuron, grad in zip(layer, layer_grad)]\n",
    "                        for layer, layer_grad in zip(network, gradients)]\n",
    "   \n",
    "            t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")\n",
    "   \n",
    "    num_correct = 0\n",
    "   \n",
    "    for n in range(1, 101):\n",
    "        x = binary_encode(n)\n",
    "        predicted = argmax(feed_forward(network, x)[-1])\n",
    "        actual = argmax(fizz_buzz_encode(n))\n",
    "        labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "        print(n, labels[predicted], labels[actual])\n",
    "   \n",
    "        if predicted == actual:\n",
    "            num_correct += 1\n",
    "   \n",
    "    print(num_correct, \"/\", 100)\n",
    "   \n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac54c4-1a3d-4070-b8cf-955be2ac73f1",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c9f3fa-6394-48dc-b7ad-6c9bd7253bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mnist\n",
      "  Using cached mnist-0.2.2-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\env2\\lib\\site-packages (from mnist) (2.0.0)\n",
      "Using cached mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
      "Installing collected packages: mnist\n",
      "Successfully installed mnist-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88d99ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\env2\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a299802-0152-46a4-91c7-b9ec4955dbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xor loss 0.000: 100%|██████████| 3000/3000 [00:04<00:00, 746.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.6425160695224112, -1.4948117798303162], [-4.567646572029666, -3.3649176350731915]]\n",
      "[1.7673716823255192, 0.3872701437947276]\n",
      "[[3.1986204791704025, -3.5018030621426206]]\n",
      "[-0.64627659633622]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fb loss: 64.51 acc: 0.95: 100%|██████████| 1000/1000 [11:28<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fb loss: 5.229 acc: 1.00: 100%|██████████| 100/100 [01:25<00:00,  1.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results 0.95\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 724\u001b[0m\n\u001b[0;32m    721\u001b[0m     dropout1\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m dropout2\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     loop(model, test_images, test_labels, loss)\n\u001b[1;32m--> 724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: main()\n",
      "Cell \u001b[1;32mIn[2], line 587\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    583\u001b[0m mnist\u001b[38;5;241m.\u001b[39mtemporary_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# Each of these functions first downloads the data and returns a numpy array.\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;66;03m# We call .tolist() because our \"tensors\" are just lists.\u001b[39;00m\n\u001b[1;32m--> 587\u001b[0m train_images \u001b[38;5;241m=\u001b[39m \u001b[43mmnist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    588\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m mnist\u001b[38;5;241m.\u001b[39mtrain_labels()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m shape(train_images) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m60000\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Dell\\env2\\Lib\\site-packages\\mnist\\__init__.py:161\u001b[0m, in \u001b[0;36mtrain_images\u001b[1;34m()\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_images\u001b[39m():\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return train images from Yann LeCun MNIST database as a numpy array.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    Download the file, if not already found in the temporary directory of\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    the system.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m        columns of the image\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdownload_and_parse_mnist_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain-images-idx3-ubyte.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\env2\\Lib\\site-packages\\mnist\\__init__.py:143\u001b[0m, in \u001b[0;36mdownload_and_parse_mnist_file\u001b[1;34m(fname, target_dir, force)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_parse_mnist_file\u001b[39m(fname, target_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download the IDX file named fname from the URL specified in dataset_url\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    and return it as a numpy array.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m        Numpy array with the dimensions and the data in the IDX file\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     fname \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     fopen \u001b[38;5;241m=\u001b[39m gzip\u001b[38;5;241m.\u001b[39mopen \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(fname)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m fopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fd:\n",
      "File \u001b[1;32mc:\\Users\\Dell\\env2\\Lib\\site-packages\\mnist\\__init__.py:59\u001b[0m, in \u001b[0;36mdownload_file\u001b[1;34m(fname, target_dir, force)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(target_fname):\n\u001b[0;32m     58\u001b[0m     url \u001b[38;5;241m=\u001b[39m urljoin(datasets_url, fname)\n\u001b[1;32m---> 59\u001b[0m     \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_fname\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:240\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    241\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:553\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    551\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    552\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 553\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:745\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    742\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    743\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 745\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\urllib\\request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tensor = list\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "assert shape([1, 2, 3]) == [3]\n",
    "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n",
    "\n",
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensonal (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "assert is_1d([1, 2, 3])\n",
    "assert not is_1d([[1, 2], [3, 4]])\n",
    "\n",
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "assert tensor_sum([1, 2, 3]) == 6\n",
    "assert tensor_sum([[1, 2], [3, 4]]) == 10\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
    "\n",
    "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\n",
    "assert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]\n",
    "\n",
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "assert zeros_like([1, 2, 3]) == [0, 0, 0]\n",
    "assert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "\n",
    "import operator\n",
    "assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
    "assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]\n",
    "\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Our neural networks will be composed of Layers, each of which\n",
    "    knows how to do some computation on its inputs in the \"forward\"\n",
    "    direction and propagate gradients in the \"backward\" direction.\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Note the lack of types. We're not going to be prescriptive\n",
    "        about what kinds of inputs layers can take and what kinds\n",
    "        of outputs they can return.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Similarly, we're not going to be prescriptive about what the\n",
    "        gradient looks like. It's up to you the user to make sure\n",
    "        that you're doing things sensibly.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the parameters of this layer. The default implementation\n",
    "        returns nothing, so that if you have a layer with no parameters\n",
    "        you don't have to implement this.\n",
    "        \"\"\"\n",
    "        return ()\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the gradients, in the same order as params()\n",
    "        \"\"\"\n",
    "        return ()\n",
    "\n",
    "from scratch.neural_networks import sigmoid\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply sigmoid to each element of the input tensor,\n",
    "        and save the results to use in backpropagation.\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                              self.sigmoids,\n",
    "                              gradient)\n",
    "\n",
    "import random\n",
    "\n",
    "from scratch.probability import inverse_normal_cdf\n",
    "\n",
    "def random_uniform(*dims: int) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
    "\n",
    "def random_normal(*dims: int,\n",
    "                  mean: float = 0.0,\n",
    "                  variance: float = 1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * inverse_normal_cdf(random.random())\n",
    "                for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
    "                for _ in range(dims[0])]\n",
    "\n",
    "assert shape(random_uniform(2, 3, 4)) == [2, 3, 4]\n",
    "assert shape(random_normal(5, 6, mean=10)) == [5, 6]\n",
    "\n",
    "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    elif init == 'xavier':\n",
    "        variance = len(dims) / sum(dims)\n",
    "        return random_normal(*dims, variance=variance)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown init: {init}\")\n",
    "\n",
    "from scratch.linear_algebra import dot\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n",
    "        \"\"\"\n",
    "        A layer of output_dim neurons, each with input_dim weights\n",
    "        (and a bias).\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # self.w[o] is the weights for the o-th neuron\n",
    "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
    "\n",
    "        # self.b[o] is the bias term for the o-th neuron\n",
    "        self.b = random_tensor(output_dim, init=init)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save the input to use in the backward pass.\n",
    "        self.input = input\n",
    "\n",
    "        # Return the vector of neuron outputs.\n",
    "        return [dot(input, self.w[o]) + self.b[o]\n",
    "                for o in range(self.output_dim)]\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        # Each b[o] gets added to output[o], which means\n",
    "        # the gradient of b is the same as the output gradient.\n",
    "        self.b_grad = gradient\n",
    "\n",
    "        # Each w[o][i] multiplies input[i] and gets added to output[o].\n",
    "        # So its gradient is input[i] * gradient[o].\n",
    "        self.w_grad = [[self.input[i] * gradient[o]\n",
    "                        for i in range(self.input_dim)]\n",
    "                       for o in range(self.output_dim)]\n",
    "\n",
    "        # Each input[i] multiplies every w[o][i] and gets added to every\n",
    "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n",
    "        # across all the outputs.\n",
    "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
    "                for i in range(self.input_dim)]\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.w, self.b]\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.w_grad, self.b_grad]\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"\n",
    "    A layer consisting of a sequence of other layers.\n",
    "    It's up to you to make sure that the output of each layer\n",
    "    makes sense as the input to the next layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Just forward the input through the layers in order.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "        return gradient\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the params from each layer.\"\"\"\n",
    "        return (param for layer in self.layers for param in layer.params())\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the grads from each layer.\"\"\"\n",
    "        return (grad for layer in self.layers for grad in layer.grads())\n",
    "\n",
    "class Loss:\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        \"\"\"How does the loss change as the predictions change?\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SSE(Loss):\n",
    "    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        # Compute the tensor of squared differences\n",
    "        squared_errors = tensor_combine(\n",
    "            lambda predicted, actual: (predicted - actual) ** 2,\n",
    "            predicted,\n",
    "            actual)\n",
    "\n",
    "        # And just add them up\n",
    "        return tensor_sum(squared_errors)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda predicted, actual: 2 * (predicted - actual),\n",
    "            predicted,\n",
    "            actual)\n",
    "\n",
    "\n",
    "sse_loss = SSE()\n",
    "assert sse_loss.loss([1, 2, 3], [10, 20, 30]) == 9 ** 2 + 18 ** 2 + 27 ** 2\n",
    "assert sse_loss.gradient([1, 2, 3], [10, 20, 30]) == [-18, -36, -54]\n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    An optimizer updates the weights of a layer (in place) using information\n",
    "    known by either the layer or the optimizer (or by both).\n",
    "    \"\"\"\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.1) -> None:\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        for param, grad in zip(layer.params(), layer.grads()):\n",
    "            # Update param using a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda param, grad: param - grad * self.lr,\n",
    "                param,\n",
    "                grad)\n",
    "\n",
    "tensor = [[1, 2], [3, 4]]\n",
    "\n",
    "for row in tensor:\n",
    "    row = [0, 0]\n",
    "assert tensor == [[1, 2], [3, 4]], \"assignment doesn't update a list\"\n",
    "\n",
    "for row in tensor:\n",
    "    row[:] = [0, 0]\n",
    "assert tensor == [[0, 0], [0, 0]], \"but slice assignment does\"\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "    def __init__(self,\n",
    "                 learning_rate: float,\n",
    "                 momentum: float = 0.9) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.mo = momentum\n",
    "        self.updates: List[Tensor] = []  # running average\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        # If we have no previous updates, start with all zeros.\n",
    "        if not self.updates:\n",
    "            self.updates = [zeros_like(grad) for grad in layer.grads()]\n",
    "\n",
    "        for update, param, grad in zip(self.updates,\n",
    "                                       layer.params(),\n",
    "                                       layer.grads()):\n",
    "            # Apply momentum\n",
    "            update[:] = tensor_combine(\n",
    "                lambda u, g: self.mo * u + (1 - self.mo) * g,\n",
    "                update,\n",
    "                grad)\n",
    "\n",
    "            # Then take a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda p, u: p - self.lr * u,\n",
    "                param,\n",
    "                update)\n",
    "\n",
    "import math\n",
    "\n",
    "def tanh(x: float) -> float:\n",
    "    # If x is very large or very small, tanh is (essentially) 1 or -1.\n",
    "    # We check for this because e.g. math.exp(1000) raises an error.\n",
    "    if x < -100:  return -1\n",
    "    elif x > 100: return 1\n",
    "\n",
    "    em2x = math.exp(-2 * x)\n",
    "    return (1 - em2x) / (1 + em2x)\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save tanh output to use in backward pass.\n",
    "        self.tanh = tensor_apply(tanh, input)\n",
    "        return self.tanh\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda tanh, grad: (1 - tanh ** 2) * grad,\n",
    "            self.tanh,\n",
    "            gradient)\n",
    "\n",
    "class Relu(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        self.input = input\n",
    "        return tensor_apply(lambda x: max(x, 0), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n",
    "                              self.input,\n",
    "                              gradient)\n",
    "\n",
    "def softmax(tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Softmax along the last dimension\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        # Subtract largest value for numerical stabilitity.\n",
    "        largest = max(tensor)\n",
    "        exps = [math.exp(x - largest) for x in tensor]\n",
    "\n",
    "        sum_of_exps = sum(exps)                 # This is the total \"weight\".\n",
    "        return [exp_i / sum_of_exps             # Probability is the fraction\n",
    "                for exp_i in exps]              # of the total weight.\n",
    "    else:\n",
    "        return [softmax(tensor_i) for tensor_i in tensor]\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    \"\"\"\n",
    "    This is the negative-log-likelihood of the observed values, given the\n",
    "    neural net model. So if we choose weights to minimize it, our model will\n",
    "    be maximizing the likelihood of the observed data.\n",
    "    \"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        # This will be log p_i for the actual class i and 0 for the other\n",
    "        # classes. We add a tiny amount to p to avoid taking log(0).\n",
    "        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n",
    "                                     probabilities,\n",
    "                                     actual)\n",
    "\n",
    "        # And then we just sum up the negatives.\n",
    "        return -tensor_sum(likelihoods)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        # Isn't this a pleasant equation?\n",
    "        return tensor_combine(lambda p, actual: p - actual,\n",
    "                              probabilities,\n",
    "                              actual)\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p: float) -> None:\n",
    "        self.p = p\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Create a mask of 0s and 1s shaped like the input\n",
    "            # using the specified probability.\n",
    "            self.mask = tensor_apply(\n",
    "                lambda _: 0 if random.random() < self.p else 1,\n",
    "                input)\n",
    "            # Multiply by the mask to dropout inputs.\n",
    "            return tensor_combine(operator.mul, input, self.mask)\n",
    "        else:\n",
    "            # During evaluation just scale down the outputs uniformly.\n",
    "            return tensor_apply(lambda x: x * (1 - self.p), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Only propagate the gradients where mask == 1\n",
    "            return tensor_combine(operator.mul, gradient, self.mask)\n",
    "        else:\n",
    "            raise RuntimeError(\"don't call backward when not in train mode\")\n",
    "\n",
    "\n",
    "#plt.savefig('im/mnist.png')\n",
    "#plt.gca().clear()\n",
    "\n",
    "def one_hot_encode(i: int, num_labels: int = 10) -> List[float]:\n",
    "    return [1.0 if j == i else 0.0 for j in range(num_labels)]\n",
    "\n",
    "assert one_hot_encode(3) == [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "assert one_hot_encode(2, num_labels=5) == [0, 0, 1, 0, 0]\n",
    "\n",
    "\n",
    "from scratch.linear_algebra import squared_distance\n",
    "\n",
    "import json\n",
    "\n",
    "def save_weights(model: Layer, filename: str) -> None:\n",
    "    weights = list(model.params())\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(weights, f)\n",
    "\n",
    "def load_weights(model: Layer, filename: str) -> None:\n",
    "    with open(filename) as f:\n",
    "        weights = json.load(f)\n",
    "\n",
    "    # Check for consistency\n",
    "    assert all(shape(param) == shape(weight)\n",
    "               for param, weight in zip(model.params(), weights))\n",
    "\n",
    "    # Then load using slice assignment:\n",
    "    for param, weight in zip(model.params(), weights):\n",
    "        param[:] = weight\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # XOR revisited\n",
    "    \n",
    "    # training data\n",
    "    xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "    ys = [[0.], [1.], [1.], [0.]]\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    net = Sequential([\n",
    "        Linear(input_dim=2, output_dim=2),\n",
    "        Sigmoid(),\n",
    "        Linear(input_dim=2, output_dim=1)\n",
    "    ])\n",
    "    \n",
    "    import tqdm\n",
    "    \n",
    "    optimizer = GradientDescent(learning_rate=0.1)\n",
    "    loss = SSE()\n",
    "    \n",
    "    with tqdm.trange(3000) as t:\n",
    "        for epoch in t:\n",
    "            epoch_loss = 0.0\n",
    "    \n",
    "            for x, y in zip(xs, ys):\n",
    "                predicted = net.forward(x)\n",
    "                epoch_loss += loss.loss(predicted, y)\n",
    "                gradient = loss.gradient(predicted, y)\n",
    "                net.backward(gradient)\n",
    "    \n",
    "                optimizer.step(net)\n",
    "    \n",
    "            t.set_description(f\"xor loss {epoch_loss:.3f}\")\n",
    "    \n",
    "    for param in net.params():\n",
    "        print(param)\n",
    "    \n",
    "    \n",
    "    # FizzBuzz Revisited\n",
    "    \n",
    "    from scratch.neural_networks import binary_encode, fizz_buzz_encode, argmax\n",
    "    \n",
    "    xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "    ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
    "    \n",
    "    NUM_HIDDEN = 25\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    net = Sequential([\n",
    "        Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
    "        Tanh(),\n",
    "        Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform'),\n",
    "        Sigmoid()\n",
    "    ])\n",
    "    \n",
    "    def fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float:\n",
    "        num_correct = 0\n",
    "        for n in range(low, hi):\n",
    "            x = binary_encode(n)\n",
    "            predicted = argmax(net.forward(x))\n",
    "            actual = argmax(fizz_buzz_encode(n))\n",
    "            if predicted == actual:\n",
    "                num_correct += 1\n",
    "    \n",
    "        return num_correct / (hi - low)\n",
    "    \n",
    "    optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
    "    loss = SSE()\n",
    "    \n",
    "    with tqdm.trange(1000) as t:\n",
    "        for epoch in t:\n",
    "            epoch_loss = 0.0\n",
    "    \n",
    "            for x, y in zip(xs, ys):\n",
    "                predicted = net.forward(x)\n",
    "                epoch_loss += loss.loss(predicted, y)\n",
    "                gradient = loss.gradient(predicted, y)\n",
    "                net.backward(gradient)\n",
    "    \n",
    "                optimizer.step(net)\n",
    "    \n",
    "            accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
    "            t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")\n",
    "    \n",
    "    # Now check results on the test set\n",
    "    print(\"test results\", fizzbuzz_accuracy(1, 101, net))\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    net = Sequential([\n",
    "        Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
    "        Tanh(),\n",
    "        Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform')\n",
    "        # No final sigmoid layer now\n",
    "    ])\n",
    "    \n",
    "    optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
    "    loss = SoftmaxCrossEntropy()\n",
    "    \n",
    "    with tqdm.trange(100) as t:\n",
    "        for epoch in t:\n",
    "            epoch_loss = 0.0\n",
    "    \n",
    "            for x, y in zip(xs, ys):\n",
    "                predicted = net.forward(x)\n",
    "                epoch_loss += loss.loss(predicted, y)\n",
    "                gradient = loss.gradient(predicted, y)\n",
    "                net.backward(gradient)\n",
    "    \n",
    "                optimizer.step(net)\n",
    "    \n",
    "            accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
    "            t.set_description(f\"fb loss: {epoch_loss:.3f} acc: {accuracy:.2f}\")\n",
    "    \n",
    "    # Again check results on the test set\n",
    "    print(\"test results\", fizzbuzz_accuracy(1, 101, net))\n",
    "    \n",
    "    \n",
    "    # Load the MNIST data\n",
    "    \n",
    "    import mnist\n",
    "    \n",
    "    # This will download the data, change this to where you want it.\n",
    "    # (Yes, it's a 0-argument function, that's what the library expects.)\n",
    "    # (Yes, I'm assigning a lambda to a variable, like I said never to do.)\n",
    "    mnist.temporary_dir = lambda: '/tmp'\n",
    "    \n",
    "    # Each of these functions first downloads the data and returns a numpy array.\n",
    "    # We call .tolist() because our \"tensors\" are just lists.\n",
    "    train_images = mnist.train_images().tolist()\n",
    "    train_labels = mnist.train_labels().tolist()\n",
    "    \n",
    "    assert shape(train_images) == [60000, 28, 28]\n",
    "    assert shape(train_labels) == [60000]\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, ax = plt.subplots(10, 10)\n",
    "    \n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            # Plot each image in black and white and hide the axes.\n",
    "            ax[i][j].imshow(train_images[10 * i + j], cmap='Greys')\n",
    "            ax[i][j].xaxis.set_visible(False)\n",
    "            ax[i][j].yaxis.set_visible(False)\n",
    "    \n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "    # Load the MNIST test data\n",
    "    \n",
    "    test_images = mnist.test_images().tolist()\n",
    "    test_labels = mnist.test_labels().tolist()\n",
    "    \n",
    "    assert shape(test_images) == [10000, 28, 28]\n",
    "    assert shape(test_labels) == [10000]\n",
    "    \n",
    "    \n",
    "    # Recenter the images\n",
    "    \n",
    "    # Compute the average pixel value\n",
    "    avg = tensor_sum(train_images) / 60000 / 28 / 28\n",
    "    \n",
    "    # Recenter, rescale, and flatten\n",
    "    train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "                    for image in train_images]\n",
    "    test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "                   for image in test_images]\n",
    "    \n",
    "    assert shape(train_images) == [60000, 784], \"images should be flattened\"\n",
    "    assert shape(test_images) == [10000, 784], \"images should be flattened\"\n",
    "    \n",
    "    # After centering, average pixel should be very close to 0\n",
    "    assert -0.0001 < tensor_sum(train_images) < 0.0001\n",
    "    \n",
    "    \n",
    "    # One-hot encode the test data\n",
    "    \n",
    "    train_labels = [one_hot_encode(label) for label in train_labels]\n",
    "    test_labels = [one_hot_encode(label) for label in test_labels]\n",
    "    \n",
    "    assert shape(train_labels) == [60000, 10]\n",
    "    assert shape(test_labels) == [10000, 10]\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    \n",
    "    import tqdm\n",
    "    \n",
    "    def loop(model: Layer,\n",
    "             images: List[Tensor],\n",
    "             labels: List[Tensor],\n",
    "             loss: Loss,\n",
    "             optimizer: Optimizer = None) -> None:\n",
    "        correct = 0         # Track number of correct predictions.\n",
    "        total_loss = 0.0    # Track total loss.\n",
    "    \n",
    "        with tqdm.trange(len(images)) as t:\n",
    "            for i in t:\n",
    "                predicted = model.forward(images[i])             # Predict.\n",
    "                if argmax(predicted) == argmax(labels[i]):       # Check for\n",
    "                    correct += 1                                 # correctness.\n",
    "                total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n",
    "    \n",
    "                # If we're training, backpropagate gradient and update weights.\n",
    "                if optimizer is not None:\n",
    "                    gradient = loss.gradient(predicted, labels[i])\n",
    "                    model.backward(gradient)\n",
    "                    optimizer.step(model)\n",
    "    \n",
    "                # And update our metrics in the progress bar.\n",
    "                avg_loss = total_loss / (i + 1)\n",
    "                acc = correct / (i + 1)\n",
    "                t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")\n",
    "    \n",
    "    \n",
    "    # The logistic regression model for MNIST\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    # Logistic regression is just a linear layer followed by softmax\n",
    "    model = Linear(784, 10)\n",
    "    loss = SoftmaxCrossEntropy()\n",
    "    \n",
    "    # This optimizer seems to work\n",
    "    optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "    \n",
    "    # Train on the training data\n",
    "    loop(model, train_images, train_labels, loss, optimizer)\n",
    "    \n",
    "    # Test on the test data (no optimizer means just evaluate)\n",
    "    loop(model, test_images, test_labels, loss)\n",
    "    \n",
    "    \n",
    "    # A deep neural network for MNIST\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    # Name them so we can turn train on and off\n",
    "    dropout1 = Dropout(0.1)\n",
    "    dropout2 = Dropout(0.1)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Linear(784, 30),  # Hidden layer 1: size 30\n",
    "        dropout1,\n",
    "        Tanh(),\n",
    "        Linear(30, 10),   # Hidden layer 2: size 10\n",
    "        dropout2,\n",
    "        Tanh(),\n",
    "        Linear(10, 10)    # Output layer: size 10\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Training the deep model for MNIST\n",
    "    \n",
    "    optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "    loss = SoftmaxCrossEntropy()\n",
    "    \n",
    "    # Enable dropout and train (takes > 20 minutes on my laptop!)\n",
    "    dropout1.train = dropout2.train = True\n",
    "    loop(model, train_images, train_labels, loss, optimizer)\n",
    "    \n",
    "    # Disable dropout and evaluate\n",
    "    dropout1.train = dropout2.train = False\n",
    "    loop(model, test_images, test_labels, loss)\n",
    "    \n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.2.1-cp312-cp312-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.53.1-cp312-cp312-win_amd64.whl.metadata (165 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\dell\\env2\\lib\\site-packages (from matplotlib) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\env2\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dell\\env2\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\env2\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.1-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.0 MB 653.6 kB/s eta 0:00:13\n",
      "   ---------------------------------------- 0.1/8.0 MB 919.0 kB/s eta 0:00:09\n",
      "    --------------------------------------- 0.1/8.0 MB 930.9 kB/s eta 0:00:09\n",
      "    --------------------------------------- 0.1/8.0 MB 654.9 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.2/8.0 MB 748.1 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.2/8.0 MB 808.4 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.2/8.0 MB 793.0 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.3/8.0 MB 710.0 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.3/8.0 MB 800.8 kB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.4/8.0 MB 829.7 kB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.5/8.0 MB 853.3 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.5/8.0 MB 879.9 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.6/8.0 MB 930.9 kB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.6/8.0 MB 910.6 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.7/8.0 MB 917.7 kB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.7/8.0 MB 909.0 kB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.8/8.0 MB 937.9 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 0.8/8.0 MB 924.5 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 0.9/8.0 MB 944.0 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 0.9/8.0 MB 967.1 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.0/8.0 MB 966.8 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.0/8.0 MB 966.8 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.0/8.0 MB 966.8 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.0/8.0 MB 876.4 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.0/8.0 MB 876.4 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.0/8.0 MB 876.4 kB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.0/8.0 MB 794.6 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.0/8.0 MB 794.6 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.0/8.0 MB 752.9 kB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.0/8.0 MB 752.9 kB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.1/8.0 MB 725.9 kB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.1/8.0 MB 716.8 kB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.1/8.0 MB 714.1 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.2/8.0 MB 744.1 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.3/8.0 MB 766.3 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.3/8.0 MB 775.7 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.4/8.0 MB 778.2 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 1.5/8.0 MB 796.4 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 1.5/8.0 MB 820.3 kB/s eta 0:00:08\n",
      "   -------- ------------------------------- 1.6/8.0 MB 843.0 kB/s eta 0:00:08\n",
      "   -------- ------------------------------- 1.7/8.0 MB 858.1 kB/s eta 0:00:08\n",
      "   -------- ------------------------------- 1.8/8.0 MB 861.7 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 1.8/8.0 MB 885.4 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 1.9/8.0 MB 888.1 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 1.9/8.0 MB 892.5 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.0/8.0 MB 897.1 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.0/8.0 MB 901.1 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.1/8.0 MB 918.3 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.2/8.0 MB 915.5 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.2/8.0 MB 920.8 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.3/8.0 MB 924.0 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.3/8.0 MB 931.3 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.4/8.0 MB 938.3 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 2.5/8.0 MB 949.3 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 2.5/8.0 MB 951.8 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 2.5/8.0 MB 938.9 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 2.6/8.0 MB 941.5 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.6/8.0 MB 942.1 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.6/8.0 MB 942.1 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.6/8.0 MB 921.7 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.7/8.0 MB 919.4 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.8/8.0 MB 919.4 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.8/8.0 MB 918.6 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.9/8.0 MB 924.9 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.9/8.0 MB 925.2 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.9/8.0 MB 923.3 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.0/8.0 MB 930.3 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.1/8.0 MB 932.9 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.1/8.0 MB 929.2 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.2/8.0 MB 931.5 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 3.2/8.0 MB 930.6 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 3.2/8.0 MB 932.3 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 3.3/8.0 MB 930.4 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 3.3/8.0 MB 938.3 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 3.4/8.0 MB 938.9 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 3.5/8.0 MB 947.8 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 3.5/8.0 MB 953.7 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 3.6/8.0 MB 955.7 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 3.6/8.0 MB 952.0 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 3.7/8.0 MB 959.0 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 3.7/8.0 MB 950.0 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 3.8/8.0 MB 955.9 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 3.9/8.0 MB 967.7 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 4.0/8.0 MB 970.3 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 4.0/8.0 MB 978.0 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 4.1/8.0 MB 979.2 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.1/8.0 MB 982.9 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.2/8.0 MB 991.4 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.3/8.0 MB 997.2 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.4/8.0 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.4/8.0 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.5/8.0 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.5/8.0 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.6/8.0 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.7/8.0 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.8/8.0 MB 1.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 4.8/8.0 MB 1.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 4.9/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.9/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.0/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.0/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.1/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.2/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.2/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.3/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.3/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.4/8.0 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.4/8.0 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.5/8.0 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.5/8.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.6/8.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.6/8.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.7/8.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.8/8.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 5.9/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.9/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.1/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.1/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.2/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.2/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.3/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.4/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.4/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.5/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.5/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.6/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.6/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.7/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.7/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.8/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.9/8.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.9/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/8.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/8.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.5/8.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.5/8.0 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/8.0 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.0/8.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.1-cp312-cp312-win_amd64.whl (189 kB)\n",
      "   ---------------------------------------- 0.0/189.9 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 61.4/189.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 163.8/189.9 kB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 184.3/189.9 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 189.9/189.9 kB 1.3 MB/s eta 0:00:00\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.1-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.2 MB 2.4 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 1.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.7/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp312-cp312-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.0 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 41.0/56.0 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.0/56.0 kB 738.1 kB/s eta 0:00:00\n",
      "Using cached pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.1 kiwisolver-1.4.5 matplotlib-3.9.1 pillow-10.4.0 pyparsing-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/matplotlib/\n",
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efd316-e829-4f1f-b153-2fae5d2b6f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-means:\n",
      "[[-25.857142857142854, -4.714285714285714], [20.0, 26.0], [16.666666666666664, 13.666666666666666]]\n",
      "\n",
      "2-means:\n",
      "[[-25.857142857142854, -4.714285714285714], [18.333333333333332, 19.833333333333332]]\n",
      "\n",
      "errors as a function of k\n",
      "1 15241.35\n",
      "2 4508.738095238095\n",
      "3 1209.0555555555554\n",
      "4 1060.0555555555554\n",
      "5 1037.8055555555554\n",
      "6 797.1666666666666\n",
      "7 475.1666666666667\n",
      "8 676.0\n",
      "9 417.0\n",
      "10 245.75\n",
      "11 255.66666666666666\n",
      "12 135.58333333333334\n",
      "13 199.25\n",
      "14 123.0\n",
      "15 50.5\n",
      "16 42.0\n",
      "17 42.5\n",
      "18 22.5\n",
      "19 2.5\n",
      "20 0.0\n",
      "\n",
      "bottom up hierarchical clustering\n",
      "(0, [(1, [(3, [(14, [(18, [([19, 28],), ([21, 27],)]), ([20, 23],)]), ([26, 13],)]), (16, [([11, 15],), ([13, 13],)])]), (2, [(4, [(5, [(9, [(11, [([-49, 0],), ([-46, 5],)]), ([-41, 8],)]), ([-49, 15],)]), ([-34, -1],)]), (6, [(7, [(8, [(10, [([-22, -16],), ([-19, -11],)]), ([-25, -9],)]), (13, [(15, [(17, [([-11, -6],), ([-12, -8],)]), ([-14, -5],)]), ([-18, -3],)])]), (12, [([-13, -19],), ([-9, -16],)])])])])\n",
      "\n",
      "three clusters, min:\n",
      "[[-49, 0], [-46, 5], [-41, 8], [-49, 15], [-34, -1], [-22, -16], [-19, -11], [-25, -9], [-11, -6], [-12, -8], [-14, -5], [-18, -3], [-13, -19], [-9, -16]]\n",
      "[[19, 28], [21, 27], [20, 23], [26, 13]]\n",
      "[[11, 15], [13, 13]]\n",
      "\n",
      "three clusters, max:\n",
      "[[11, 15], [13, 13], [26, 13], [19, 28], [21, 27], [20, 23]]\n",
      "[[-41, 8], [-49, 15], [-49, 0], [-46, 5], [-34, -1]]\n",
      "[[-11, -6], [-12, -8], [-14, -5], [-18, -3], [-22, -16], [-19, -11], [-25, -9], [-13, -19], [-9, -16]]\n"
     ]
    }
   ],
   "source": [
    "from scratch import linear_algebra\n",
    "from scratch.linear_algebra import squared_distance, vector_mean, distance\n",
    "import math, random\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class KMeans:\n",
    "    \"\"\"performs k-means clustering\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k          # number of clusters\n",
    "        self.means = None   # means of clusters\n",
    "\n",
    "    def classify(self, input):\n",
    "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
    "        return min(range(self.k),\n",
    "                   key=lambda i: squared_distance(input, self.means[i]))\n",
    "\n",
    "    def train(self, inputs):\n",
    "\n",
    "        self.means = random.sample(inputs, self.k)\n",
    "        assignments = None\n",
    "\n",
    "        while True:\n",
    "            # Find new assignments\n",
    "            new_assignments = list(map(self.classify, inputs))\n",
    "\n",
    "            # If no assignments have changed, we're done.\n",
    "            if assignments == new_assignments:\n",
    "                return\n",
    "\n",
    "            # Otherwise keep the new assignments,\n",
    "            assignments = new_assignments\n",
    "\n",
    "            for i in range(self.k):\n",
    "                i_points = [p for p, a in zip(inputs, assignments) if a == i]\n",
    "                # avoid divide-by-zero if i_points is empty\n",
    "                if i_points:\n",
    "                    self.means[i] = vector_mean(i_points)\n",
    "\n",
    "def squared_clustering_errors(inputs, k):\n",
    "    \"\"\"finds the total squared error from k-means clustering the inputs\"\"\"\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(inputs)\n",
    "    means = clusterer.means\n",
    "    assignments = list(map(clusterer.classify, inputs))\n",
    "\n",
    "    return sum(squared_distance(input,means[cluster])\n",
    "               for input, cluster in zip(inputs, assignments))\n",
    "\n",
    "def plot_squared_clustering_errors():\n",
    "\n",
    "    ks = range(1, len(inputs) + 1)\n",
    "    errors = [squared_clustering_errors(inputs, k) for k in ks]\n",
    "\n",
    "    plt.plot(ks, errors)\n",
    "    plt.xticks(ks)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"total squared error\")\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# using clustering to recolor an image\n",
    "#\n",
    "\n",
    "def recolor_image(input_file, k=5):\n",
    "\n",
    "    img = mpimg.imread(path_to_png_file)\n",
    "    pixels = [pixel for row in img for pixel in row]\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(pixels) # this might take a while\n",
    "\n",
    "    def recolor(pixel):\n",
    "        cluster = clusterer.classify(pixel) # index of the closest cluster\n",
    "        return clusterer.means[cluster]     # mean of the closest cluster\n",
    "\n",
    "    new_img = [[recolor(pixel) for pixel in row]\n",
    "               for row in img]\n",
    "\n",
    "    plt.imshow(new_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# hierarchical clustering\n",
    "#\n",
    "\n",
    "def is_leaf(cluster):\n",
    "    \"\"\"a cluster is a leaf if it has length 1\"\"\"\n",
    "    return len(cluster) == 1\n",
    "\n",
    "def get_children(cluster):\n",
    "    \"\"\"returns the two children of this cluster if it's a merged cluster;\n",
    "    raises an exception if this is a leaf cluster\"\"\"\n",
    "    if is_leaf(cluster):\n",
    "        raise TypeError(\"a leaf cluster has no children\")\n",
    "    else:\n",
    "        return cluster[1]\n",
    "\n",
    "def get_values(cluster):\n",
    "    \"\"\"returns the value in this cluster (if it's a leaf cluster)\n",
    "    or all the values in the leaf clusters below it (if it's not)\"\"\"\n",
    "    if is_leaf(cluster):\n",
    "        return cluster # is already a 1-tuple containing value\n",
    "    else:\n",
    "        return [value\n",
    "                for child in get_children(cluster)\n",
    "                for value in get_values(child)]\n",
    "\n",
    "def cluster_distance(cluster1, cluster2, distance_agg=min):\n",
    "    \"\"\"finds the aggregate distance between elements of cluster1\n",
    "    and elements of cluster2\"\"\"\n",
    "    return distance_agg([distance(input1, input2)\n",
    "                        for input1 in get_values(cluster1)\n",
    "                        for input2 in get_values(cluster2)])\n",
    "\n",
    "def get_merge_order(cluster):\n",
    "    if is_leaf(cluster):\n",
    "        return float('inf')\n",
    "    else:\n",
    "        return cluster[0] # merge_order is first element of 2-tuple\n",
    "\n",
    "def bottom_up_cluster(inputs, distance_agg=min):\n",
    "    # start with every input a leaf cluster / 1-tuple\n",
    "    clusters = [(input,) for input in inputs]\n",
    "\n",
    "    # as long as we have more than one cluster left...\n",
    "    while len(clusters) > 1:\n",
    "        # find the two closest clusters\n",
    "        c1, c2 = min([(cluster1, cluster2)\n",
    "                     for i, cluster1 in enumerate(clusters)\n",
    "                     for cluster2 in clusters[:i]],\n",
    "                     key=lambda p: cluster_distance(p[0], p[1], distance_agg))\n",
    "\n",
    "        # remove them from the list of clusters\n",
    "        clusters = [c for c in clusters if c != c1 and c != c2]\n",
    "\n",
    "        # merge them, using merge_order = # of clusters left\n",
    "        merged_cluster = (len(clusters), [c1, c2])\n",
    "\n",
    "        # and add their merge\n",
    "        clusters.append(merged_cluster)\n",
    "\n",
    "    # when there's only one cluster left, return it\n",
    "    return clusters[0]\n",
    "\n",
    "def generate_clusters(base_cluster, num_clusters):\n",
    "    # start with a list with just the base cluster\n",
    "    clusters = [base_cluster]\n",
    "\n",
    "    # as long as we don't have enough clusters yet...\n",
    "    while len(clusters) < num_clusters:\n",
    "        # choose the last-merged of our clusters\n",
    "        next_cluster = min(clusters, key=get_merge_order)\n",
    "        # remove it from the list\n",
    "        clusters = [c for c in clusters if c != next_cluster]\n",
    "        # and add its children to the list (i.e., unmerge it)\n",
    "        clusters.extend(get_children(next_cluster))\n",
    "\n",
    "    # once we have enough clusters...\n",
    "    return clusters\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    inputs = [[-14,-5],[13,13],[20,23],[-19,-11],[-9,-16],[21,27],[-49,15],[26,13],[-46,5],[-34,-1],[11,15],[-49,0],[-22,-16],[19,28],[-12,-8],[-13,-19],[-41,8],[-11,-6],[-25,-9],[-18,-3]]\n",
    "\n",
    "    random.seed(0) # so you get the same results as me\n",
    "    clusterer = KMeans(3)\n",
    "    clusterer.train(inputs)\n",
    "    print(\"3-means:\")\n",
    "    print(clusterer.means)\n",
    "    print()\n",
    "\n",
    "    random.seed(0)\n",
    "    clusterer = KMeans(2)\n",
    "    clusterer.train(inputs)\n",
    "    print(\"2-means:\")\n",
    "    print(clusterer.means)\n",
    "    print()\n",
    "\n",
    "    print(\"errors as a function of k\")\n",
    "\n",
    "    for k in range(1, len(inputs) + 1):\n",
    "        print(k, squared_clustering_errors(inputs, k))\n",
    "    print()\n",
    "\n",
    "\n",
    "    print(\"bottom up hierarchical clustering\")\n",
    "\n",
    "    base_cluster = bottom_up_cluster(inputs)\n",
    "    print(base_cluster)\n",
    "\n",
    "    print()\n",
    "    print(\"three clusters, min:\")\n",
    "    for cluster in generate_clusters(base_cluster, 3):\n",
    "        print(get_values(cluster))\n",
    "\n",
    "    print()\n",
    "    print(\"three clusters, max:\")\n",
    "    base_cluster = bottom_up_cluster(inputs, max)\n",
    "    for cluster in generate_clusters(base_cluster, 3):\n",
    "        print(get_values(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e0505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
